package com.sciencepixel.service

import com.sciencepixel.domain.VideoStatus
import com.sciencepixel.repository.SystemSettingRepository
import com.sciencepixel.repository.VideoHistoryRepository
import org.springframework.batch.core.Job
import org.springframework.batch.core.JobParametersBuilder
import org.springframework.batch.core.launch.JobLauncher
import org.springframework.scheduling.annotation.Scheduled
import org.springframework.stereotype.Service
import java.util.Date

@Service
class BatchScheduler(
    private val jobLauncher: JobLauncher,
    private val shortsJob: Job,
    private val videoHistoryRepository: VideoHistoryRepository,
    private val systemSettingRepository: SystemSettingRepository,
    private val cleanupService: CleanupService,
    private val kafkaEventPublisher: com.sciencepixel.event.KafkaEventPublisher,
    @org.springframework.beans.factory.annotation.Value("\${SHORTS_CHANNEL_ID:science}") private val channelId: String
) {

    // Îß§ 10Î∂ÑÎßàÎã§ Ïã§Ìñâ (0, 10, 20, 30, 40, 50Î∂Ñ)
    @Scheduled(cron = "\${app.scheduling.batch-cron:0 0/10 * * * *}")
    fun runBatchJobIfNeeded() {
        println("‚è∞ Batch Scheduler: Checking generation buffer at ${Date()}")

        // 1. Pre-Cleanup: 1ÏãúÍ∞Ñ Ïù¥ÏÉÅ Í≤ΩÍ≥ºÌïú 'ÏûëÏóÖ Ï§ë' Î†àÏΩîÎìú ÏÇ≠Ï†ú
        try {
            cleanupService.cleanupStaleJobs()
        } catch (e: Exception) {
            println("‚ö†Ô∏è Stale Job Cleanup Warning: ${e.message}")
        }

        // 2. Get Limit from Settings (Default 10)
        val limit = systemSettingRepository.findByChannelIdAndKey(channelId, "MAX_GENERATION_LIMIT")
            ?.value?.toIntOrNull() ?: 10

        // 3. Count Active/Pending videos
        val excludedStatuses = listOf(
            VideoStatus.UPLOADED, 
            VideoStatus.FAILED
        )
        val activeCount = videoHistoryRepository.findByChannelIdAndStatusNotIn(channelId, excludedStatuses).size

        println("üìä Active/Pending Video Buffer: $activeCount / $limit")

        if (activeCount < limit) {
            println("üöÄ Buffer low. Triggering Batch Job (Throttled to 1 item)...")
            try {
                // User requested 1 generation per cycle (10 min)
                // Even if we have many slots, we only schedule 1.
                val remaining = 1L 
                
                val params = JobParametersBuilder()
                    .addLong("time", System.currentTimeMillis())
                    .addLong("remainingSlots", remaining)
                    .toJobParameters()
                
                jobLauncher.run(shortsJob, params)
            } catch (e: Exception) {
                println("‚ùå Batch Job Launch Failed: ${e.message}")
            }
        } else {
            println("üõë Buffer Full (>= $limit). Skipping generation.")
        }
    }

    // Îß§Ïãú 5Î∂ÑÏóê Ïã§Ìå®Ìïú ÏòÅÏÉÅ Ïû¨ÏãúÎèÑ Ï≤¥ÌÅ¨ (0 5 * * * *)
    @Scheduled(cron = "0 5 * * * *")
    fun retryFailedGenerations() {
        println("‚è∞ Batch Scheduler: Checking for FAILED videos to retry at ${Date()}")
        
        val failedVideos = videoHistoryRepository.findByChannelIdAndStatus(channelId, VideoStatus.FAILED)
            .filter { it.regenCount < 1 } // Ïû¨ÏÉùÏÑ± ÏãúÎèÑ Ïïà Ìïú Í≤ÉÎßå
        
        if (failedVideos.isNotEmpty()) {
            println("üîÑ Found ${failedVideos.size} FAILED videos. Analyzing failure steps...")
            
            failedVideos.take(5).forEach { video ->
                if (video.failureStep == "UPLOAD") {
                    val file = java.io.File(video.filePath)
                    if (file.exists() && file.length() > 0) {
                        println("‚ôªÔ∏è [Auto-Recovery] File exists for ${video.title} (UPLOAD fail). Resetting to COMPLETED for retry.")
                        videoHistoryRepository.save(video.copy(
                            status = VideoStatus.COMPLETED,
                            failureStep = "",
                            errorMessage = "",
                            updatedAt = java.time.LocalDateTime.now()
                        ))
                        return@forEach
                    }
                }
                
                // Default: Full Regeneration
                println("üîÑ [$channelId] [Auto-Recovery] Triggering full regeneration for: ${video.title}")
                kafkaEventPublisher.publishRegenerationRequested(
                    com.sciencepixel.event.RegenerationRequestedEvent(
                        channelId = channelId, // Ï∂îÍ∞Ä
                        videoId = video.id!!,
                        title = video.title,
                        summary = video.summary,
                        link = video.link,
                        regenCount = video.regenCount
                    )
                )
            }
        }
    }
    // Îß§Ïãú 0Î∂ÑÏóê ÏóÖÎ°úÎìú Ï≤¥ÌÅ¨ (0 0 * * * *)
    @Scheduled(cron = "0 0 * * * *")
    fun scheduleUploads() {
        println("‚è∞ Batch Scheduler: Checking Upload Schedule for [$channelId] at ${Date()}")

        // 1. Determine Upload Interval
        // Stocks, History -> 1 per day (24 hours min interval, or just once per calendar day?)
        // Science, Horror -> 1 per hour
        val minIntervalHours = when(channelId) {
            "stocks", "history" -> 24L
            else -> 1L
        }

        // 2. Check Last Upload Time
        val lastUploaded = videoHistoryRepository.findFirstByChannelIdAndStatusOrderByUpdatedAtDesc(channelId, VideoStatus.UPLOADED)
        val now = java.time.LocalDateTime.now()
        
        if (lastUploaded != null) {
            val hoursSinceLastUpload = java.time.temporal.ChronoUnit.HOURS.between(lastUploaded.updatedAt, now)
            if (hoursSinceLastUpload < minIntervalHours) {
                println("‚è≥ [$channelId] Upload skipped. Last upload was $hoursSinceLastUpload hours ago (Min Interval: $minIntervalHours hrs).")
                return
            }
        } else {
             println("üÜï [$channelId] No previous uploads found. Proceeding with first upload.")
        }

        // 3. Find Next Ready Video (FIFO)
        // Find oldest COMPLETED video
        val nextVideo = videoHistoryRepository.findAllByChannelIdOrderByCreatedAtDesc(
            channelId, 
            org.springframework.data.domain.PageRequest.of(0, 100) // Sort Descending to get list, then we pick Last (Oldest)?? 
            // Better: Find findFirstByChannelIdAndStatusOrderByCreatedAtAsc
        )
        // Since we don't have 'findFirstBy...Asc' readily exposed in repo snippets above, let's look at available methods.
        // We can fetch list by findAllByChannelIdOrderByCreatedAtDesc and take the LAST one.
        
        // Let's add specific method to repo if needed, OR use existing.
        // Assuming we can use streams or just add the method.
        // Let's check 'videoHistoryRepository' methods.
        // We have 'findAllByChannelIdOrderByCreatedAtDesc'.
        // So the last item is the oldest.
        
        val completedVideos = videoHistoryRepository.findByChannelIdAndStatus(channelId, VideoStatus.COMPLETED)
            .sortedBy { it.createdAt } // Oldest first
        
        if (completedVideos.isNotEmpty()) {
            val videoToUpload = completedVideos.first()
            println("üöÄ [$channelId] Triggering Upload for: ${videoToUpload.title}")
            
            // Publish Upload Requested Event
            kafkaEventPublisher.publishUploadRequested(
                com.sciencepixel.event.UploadRequestedEvent(
                    channelId = channelId,
                    videoId = videoToUpload.id!!,
                    title = videoToUpload.title,
                    filePath = videoToUpload.filePath
                )
            )
            
            // Optimistic Update to prevent double scheduling if frequent checks?
            // VideoUploadConsumer will handle Locking (UPLOADING status).
            
        } else {
             println("üìâ [$channelId] No COMPLETED videos ready for upload.")
        }
    }
}
